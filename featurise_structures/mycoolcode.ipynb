{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to make a fake dataset here to be able to test how featurising these bravais lattices will work with PHF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import structures as st\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.diagrams import PersistenceEntropy\n",
    "from gtda.diagrams import NumberOfPoints\n",
    "from gtda.diagrams import Amplitude\n",
    "\n",
    "from sklearn.pipeline import make_union, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persistence_diagrams(coords):\n",
    "    # Gives data to make persitenace diagrams, need to make code for that\n",
    "    # Track connected components, loops, and voids\n",
    "    homology_dimensions = [0, 1, 2]\n",
    "\n",
    "    # Collapse edges to speed up H2 persistence calculation!\n",
    "    persistence = VietorisRipsPersistence(\n",
    "        metric=\"euclidean\",\n",
    "        homology_dimensions=homology_dimensions,\n",
    "        n_jobs=1,\n",
    "        collapse_edges=True,\n",
    "    )\n",
    "    \n",
    "    reshaped_coords=coords[None, :, :]\n",
    "    diagrams_basic = persistence.fit_transform(reshaped_coords)\n",
    "    return coords, diagrams_basic\n",
    "\n",
    "def make_pipeline():\n",
    "\n",
    "    metrics = [\n",
    "        {\"metric\": metric}\n",
    "        for metric in [\"bottleneck\", \"wasserstein\", \"landscape\", \"persistence_image\"]\n",
    "    ]\n",
    "\n",
    "    # Concatenate to generate 3 + 3 + (4 x 3) = 18 topological features\n",
    "    feature_union = make_union(\n",
    "        PersistenceEntropy(normalize=True),\n",
    "        NumberOfPoints(n_jobs=1),\n",
    "        *[Amplitude(**metric, n_jobs=1) for metric in metrics]\n",
    "    )\n",
    "\n",
    "    ## then we use a pipeline to transform, the data and spit i out\n",
    "    # mwah hahahahaha\n",
    "    pipe = Pipeline(\n",
    "        [\n",
    "            (\"features\", feature_union)\n",
    "        ]\n",
    "    )\n",
    "        \n",
    "    return pipe\n",
    "\n",
    "def featurising_coords(coords_of_structures):\n",
    "    topol_feat_list = []\n",
    "    pipe = make_pipeline()\n",
    "\n",
    "    for coords in coords_of_structures:\n",
    "        _ , diagrams_basic = persistence_diagrams(coords)\n",
    "        X_basic = pipe.fit_transform(diagrams_basic)\n",
    "        # topology feat list stores the topological features for each structure\n",
    "        topol_feat_list.append([x for x in X_basic[0]])\n",
    "    \n",
    "    # topol feat mat is a matrix of topological features\n",
    "    topol_feat_mat = np.array(topol_feat_list)\n",
    "    \n",
    "    return topol_feat_mat, topol_feat_list\n",
    "\n",
    "def do_random_forest(features, target):\n",
    "    # we need to binarize the labels\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    features_transformed = mlb.fit_transform(features)\n",
    "\n",
    "    x_train, x_test ,y_train, y_test = train_test_split(features_transformed,target,test_size=0.2, random_state=42)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    accuracy = round(model.score(x_test, y_test) * 100, 2)\n",
    "    print(f\"Accuracy of the random forest: {accuracy}%\")\n",
    "    \n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    \n",
    "    return f1, precision, recall, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomise_sides(length, num_of_diff_sides):\n",
    "    sides_df = pd.DataFrame(columns=['a', 'b', 'c'])\n",
    "    \n",
    "    while len(sides_df) < length:\n",
    "        side_a = round(np.random.uniform(3, 9.0), 3)\n",
    "        if num_of_diff_sides == 1:\n",
    "            side_b = side_a\n",
    "            side_c  = round(np.random.uniform(3, 9.0), 3)\n",
    "            \n",
    "            if side_a != side_c:\n",
    "                sides_df = sides_df.append({'a': side_a, 'b': side_b, 'c': side_c}, ignore_index=True)\n",
    "            \n",
    "        elif num_of_diff_sides == 2:\n",
    "            side_b = round(np.random.uniform(3, 9.0), 3)\n",
    "            side_c  = round(np.random.uniform(3, 9.0), 3)\n",
    "            \n",
    "            if side_a != side_b != side_c:\n",
    "                sides_df = sides_df.append({'a': side_a, 'b': side_b, 'c': side_c}, ignore_index=True)\n",
    "                \n",
    "        else:\n",
    "            side_b = side_a\n",
    "            side_c = side_a\n",
    "            sides_df = sides_df.append({'a': side_a, 'b': side_b, 'c': side_c}, ignore_index=True)\n",
    "            \n",
    "    return sides_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomise_angles(length, randomise_1, randomise_3, same_3_randomised):\n",
    "    if randomise_1 == True:\n",
    "        angles = []\n",
    "        while len(angles) < length:\n",
    "            ang_a = np.random.uniform(45, 80)\n",
    "            ang_a = round(ang_a, 1)\n",
    "            angles.append(ang_a)\n",
    "            \n",
    "    elif randomise_3 == True:\n",
    "        angles = []\n",
    "        while len(angles) < length:\n",
    "            ang_a = np.random.uniform(45, 80)\n",
    "            ang_a = round(ang_a, 1)\n",
    "            ang_b = np.random.uniform(45, 80)\n",
    "            ang_b = round(ang_b, 1)\n",
    "            ang_y = np.random.uniform(45, 80)\n",
    "            ang_y = round(ang_y, 1)\n",
    "            \n",
    "            if ang_a != ang_b != ang_y:\n",
    "                angles.append([ang_a, ang_b, ang_y])\n",
    "                \n",
    "    elif same_3_randomised == True:\n",
    "        angles = []\n",
    "        while len(angles) < length:\n",
    "            ang_a = np.random.uniform(45, 80)\n",
    "            ang_a = round(ang_a, 1)\n",
    "            ang_b = ang_a\n",
    "            ang_y = ang_a\n",
    "            \n",
    "            angles.append([ang_a, ang_b, ang_y])\n",
    "            \n",
    "    else:\n",
    "        print('Please choose a valid option')\n",
    "        \n",
    "    return angles\n",
    "\n",
    "# try_ang = randomise_angles(10, False, True, False)\n",
    "# try_ang_df = pd.DataFrame(try_ang, columns=['angle_a', 'angle_b', 'angle_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(a, b, c, alpha, beta, gamma, name):\n",
    "    data = {\n",
    "        'a': a,\n",
    "        'b': b,\n",
    "        'c': c,\n",
    "        'alpha': alpha,\n",
    "        'beta': beta,\n",
    "        'gamma': gamma,\n",
    "        'structure type': name\n",
    "        }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 2 100\n",
      "Accuracy of the random forest: 25.71%\n",
      "F1 Score: 0.11670197272606911\n",
      "Precision: 0.24386128364389234\n",
      "Recall: 0.2571428571428571\n",
      "700 3 100\n",
      "Accuracy of the random forest: 25.71%\n",
      "F1 Score: 0.11904516417633615\n",
      "Precision: 0.21049676351046215\n",
      "Recall: 0.2571428571428571\n",
      "700 4 100\n",
      "Accuracy of the random forest: 25.71%\n",
      "F1 Score: 0.11628919860627178\n",
      "Precision: 0.22209728962007338\n",
      "Recall: 0.2571428571428571\n",
      "1400 2 200\n",
      "Accuracy of the random forest: 28.21%\n",
      "F1 Score: 0.134392698176276\n",
      "Precision: 0.22769302615193027\n",
      "Recall: 0.28214285714285714\n",
      "1400 3 200\n",
      "Accuracy of the random forest: 29.29%\n",
      "F1 Score: 0.16156138205705475\n",
      "Precision: 0.3440707539122478\n",
      "Recall: 0.29285714285714287\n",
      "1400 4 200\n",
      "Accuracy of the random forest: 27.86%\n",
      "F1 Score: 0.13252734955064766\n",
      "Precision: 0.1907510876691102\n",
      "Recall: 0.2785714285714286\n",
      "2800 2 400\n",
      "Accuracy of the random forest: 28.21%\n",
      "F1 Score: 0.14335704983440403\n",
      "Precision: 0.2958021677570549\n",
      "Recall: 0.28214285714285714\n",
      "2800 3 400\n",
      "Accuracy of the random forest: 28.57%\n",
      "F1 Score: 0.15507931641367975\n",
      "Precision: 0.587008337725241\n",
      "Recall: 0.2857142857142857\n",
      "2800 4 400\n",
      "Accuracy of the random forest: 29.29%\n",
      "F1 Score: 0.1625106070037559\n",
      "Precision: 0.5682770445134575\n",
      "Recall: 0.29285714285714287\n",
      "4200 2 600\n",
      "Accuracy of the random forest: 30.36%\n",
      "F1 Score: 0.18848563414782757\n",
      "Precision: 0.5812412537231706\n",
      "Recall: 0.30357142857142855\n",
      "4200 3 600\n",
      "Accuracy of the random forest: 31.67%\n",
      "F1 Score: 0.21358669310468983\n",
      "Precision: 0.5438533156484284\n",
      "Recall: 0.31666666666666665\n",
      "4200 4 600\n",
      "Accuracy of the random forest: 30.71%\n",
      "F1 Score: 0.20146815785560343\n",
      "Precision: 0.5232319796622862\n",
      "Recall: 0.30714285714285716\n",
      "5600 2 800\n",
      "Accuracy of the random forest: 30.18%\n",
      "F1 Score: 0.17987704885466727\n",
      "Precision: 0.46771076550406077\n",
      "Recall: 0.30178571428571427\n",
      "5600 3 800\n",
      "Accuracy of the random forest: 32.14%\n",
      "F1 Score: 0.21366310005727793\n",
      "Precision: 0.5772307581331936\n",
      "Recall: 0.32142857142857145\n",
      "5600 4 800\n",
      "Accuracy of the random forest: 30.27%\n",
      "F1 Score: 0.177015274780808\n",
      "Precision: 0.5593330797267211\n",
      "Recall: 0.3026785714285714\n",
      "7000 2 1000\n",
      "Accuracy of the random forest: 28.71%\n",
      "F1 Score: 0.17548961448123856\n",
      "Precision: 0.37045904329919616\n",
      "Recall: 0.28714285714285714\n",
      "7000 3 1000\n",
      "Accuracy of the random forest: 29.79%\n",
      "F1 Score: 0.1993128845769251\n",
      "Precision: 0.4654247841790358\n",
      "Recall: 0.2978571428571429\n",
      "7000 4 1000\n",
      "Accuracy of the random forest: 31.14%\n",
      "F1 Score: 0.20655342285963843\n",
      "Precision: 0.5311638452453317\n",
      "Recall: 0.31142857142857144\n"
     ]
    }
   ],
   "source": [
    "lengths = [100, 200, 400, 600, 800, 1000]\n",
    "unit_cell_size = [2, 3, 4]\n",
    "for length in lengths:\n",
    "    for size in unit_cell_size:\n",
    "        sides = randomise_sides(length, 0)\n",
    "        cubic = make_dataset(sides[\"a\"], sides[\"b\"], sides[\"c\"], 90, 90, 90, \"cubic\")\n",
    "\n",
    "        sides = randomise_sides(length, 0)\n",
    "        hexagonal = make_dataset(sides[\"a\"], sides[\"b\"], sides[\"c\"], 90, 90, 120, \"hexagonal\")\n",
    "\n",
    "        sides = randomise_sides(length, 1)\n",
    "        tetragonal = make_dataset(sides[\"a\"], sides[\"b\"], sides[\"c\"], 90, 90, 90, \"tetragonal\")\n",
    "\n",
    "        sides = randomise_sides(length, 2)\n",
    "        orthorhombic = make_dataset(sides[\"a\"], sides[\"b\"], sides[\"c\"], 90, 90, 90, \"orthorhombic\")\n",
    "\n",
    "        sides = randomise_sides(length, 0)\n",
    "        angles = randomise_angles(length, False, False, True)\n",
    "        angles = pd.DataFrame(angles, columns=['alpha', 'beta', 'gamma'])\n",
    "        rhomobohedral = make_dataset(sides[\"a\"], sides[\"b\"], sides[\"c\"], angles[\"alpha\"], \n",
    "                                    angles[\"beta\"], angles[\"gamma\"], \"rhomobohedral\")\n",
    "\n",
    "        sides = randomise_sides(length, 2)\n",
    "        angles = randomise_angles(length, True, False, False)\n",
    "        monoclinc = make_dataset(sides[\"a\"], sides[\"b\"], sides[\"c\"], 90, angles, 90, \"monoclinc\")\n",
    "\n",
    "        sides = randomise_sides(length, 2)\n",
    "        angles = randomise_angles(length, False, True, False)\n",
    "        angles = pd.DataFrame(angles, columns=['alpha', 'beta', 'gamma'])\n",
    "\n",
    "        triclinic = make_dataset(sides[\"a\"], sides[\"b\"], sides[\"c\"],\n",
    "                                    angles[\"alpha\"], angles[\"beta\"], angles[\"gamma\"], \"triclinic\")\n",
    "\n",
    "        final_df = pd.concat([cubic, hexagonal, rhomobohedral, tetragonal, orthorhombic, monoclinc, triclinic],ignore_index=True)\n",
    "\n",
    "        coords = []\n",
    "        for index, row in final_df.iterrows():\n",
    "            coords.append(st.Structure(size,size,size,\n",
    "                                    row[\"a\"] ,row[\"b\"], row[\"c\"], \n",
    "                                    row[\"alpha\"], row[\"beta\"], row[\"gamma\"],\n",
    "                                    False, False, False))\n",
    "            \n",
    "        matrix_list, feat_cryst_list = featurising_coords(coords_of_structures=coords)\n",
    "        final_df['Crystals Featurised'] = feat_cryst_list\n",
    "\n",
    "        final_df[\"Lowest distortion\"] = final_df[\"structure type\"].astype('category')\n",
    "        final_df[\"Lowest distortion Categories\"] = final_df[\"Lowest distortion\"].cat.codes\n",
    "        \n",
    "        print(len(final_df), size, length)\n",
    "        f1, precision, recall, accuracy = do_random_forest(final_df[\"Crystals Featurised\"], final_df[\"Lowest distortion Categories\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
